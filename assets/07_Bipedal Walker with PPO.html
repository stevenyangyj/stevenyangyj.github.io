<!DOCTYPE html>
<html>
<head>
<title>07_Bipedal Walker with PPO.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="control-bipedal-walker-with-ppo">Control Bipedal Walker With PPO</h1>
<div>
<img src=https://gymnasium.farama.org/_images/bipedal_walker.gif width=40%/>
</div>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> scipy.signal
<span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> Box, Discrete
<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam
<span class="hljs-keyword">from</span> torch.distributions.normal <span class="hljs-keyword">import</span> Normal
<span class="hljs-keyword">from</span> torch.distributions.categorical <span class="hljs-keyword">import</span> Categorical
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">combined_shape</span><span class="hljs-params">(length, shape=None)</span>:</span>
    <span class="hljs-keyword">if</span> shape <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> (length,)
    <span class="hljs-keyword">return</span> (length, shape) <span class="hljs-keyword">if</span> np.isscalar(shape) <span class="hljs-keyword">else</span> (length, *shape)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mlp</span><span class="hljs-params">(sizes, activation, output_activation=nn.Identity)</span>:</span>
    layers = []
    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(sizes)<span class="hljs-number">-1</span>):
        act = activation <span class="hljs-keyword">if</span> j &lt; len(sizes)<span class="hljs-number">-2</span> <span class="hljs-keyword">else</span> output_activation
        layers += [nn.Linear(sizes[j], sizes[j+<span class="hljs-number">1</span>]), act()]
    <span class="hljs-keyword">return</span> nn.Sequential(*layers)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">count_vars</span><span class="hljs-params">(module)</span>:</span>
    <span class="hljs-keyword">return</span> sum([np.prod(p.shape) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> module.parameters()])


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discount_cumsum</span><span class="hljs-params">(x, discount)</span>:</span>
    <span class="hljs-string">"""
    magic from rllab for computing discounted cumulative sums of vectors.
    input: 
        vector x, 
        [x0, 
         x1, 
         x2]
    output:
        [x0 + discount * x1 + discount^2 * x2,  
         x1 + discount * x2,
         x2]
    """</span>
    <span class="hljs-keyword">return</span> scipy.signal.lfilter([<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, float(-discount)], x[::<span class="hljs-number">-1</span>], axis=<span class="hljs-number">0</span>)[::<span class="hljs-number">-1</span>]


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actor</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_distribution</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">raise</span> NotImplementedError

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_log_prob_from_distribution</span><span class="hljs-params">(self, pi, act)</span>:</span>
        <span class="hljs-keyword">raise</span> NotImplementedError

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, obs, act=None)</span>:</span>
        <span class="hljs-comment"># Produce action distributions for given observations, and </span>
        <span class="hljs-comment"># optionally compute the log likelihood of given actions under</span>
        <span class="hljs-comment"># those distributions.</span>
        pi = self._distribution(obs)
        logp_a = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">if</span> act <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            logp_a = self._log_prob_from_distribution(pi, act)
        <span class="hljs-keyword">return</span> pi, logp_a


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPCategoricalActor</span><span class="hljs-params">(Actor)</span>:</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, act_dim, hidden_sizes, activation)</span>:</span>
        super().__init__()
        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_distribution</span><span class="hljs-params">(self, obs)</span>:</span>
        logits = self.logits_net(obs)
        <span class="hljs-keyword">return</span> Categorical(logits=logits)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_log_prob_from_distribution</span><span class="hljs-params">(self, pi, act)</span>:</span>
        <span class="hljs-keyword">return</span> pi.log_prob(act)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPGaussianActor</span><span class="hljs-params">(Actor)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, act_dim, hidden_sizes, activation)</span>:</span>
        super().__init__()
        log_std = <span class="hljs-number">-0.5</span> * np.ones(act_dim, dtype=np.float32)
        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))
        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_distribution</span><span class="hljs-params">(self, obs)</span>:</span>
        mu = self.mu_net(obs)
        std = torch.exp(self.log_std)
        <span class="hljs-keyword">return</span> Normal(mu, std)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_log_prob_from_distribution</span><span class="hljs-params">(self, pi, act)</span>:</span>
        <span class="hljs-keyword">return</span> pi.log_prob(act).sum(axis=<span class="hljs-number">-1</span>)    <span class="hljs-comment"># Last axis sum needed for Torch Normal distribution</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPCritic</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, hidden_sizes, activation)</span>:</span>
        super().__init__()
        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [<span class="hljs-number">1</span>], activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">return</span> torch.squeeze(self.v_net(obs), <span class="hljs-number">-1</span>) <span class="hljs-comment"># Critical to ensure v has right shape.</span>



<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPActorCritic</span><span class="hljs-params">(nn.Module)</span>:</span>


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, observation_space, action_space, 
                 hidden_sizes=<span class="hljs-params">(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>)</span>, activation=nn.Tanh)</span>:</span>
        super().__init__()

        obs_dim = observation_space.shape[<span class="hljs-number">0</span>]

        <span class="hljs-comment"># policy builder depends on action space</span>
        <span class="hljs-keyword">if</span> isinstance(action_space, Box):
            self.pi = MLPGaussianActor(obs_dim, action_space.shape[<span class="hljs-number">0</span>], hidden_sizes, activation)
        <span class="hljs-keyword">elif</span> isinstance(action_space, Discrete):
            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)

        <span class="hljs-comment"># build value function</span>
        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">with</span> torch.no_grad():
            pi = self.pi._distribution(obs)
            a = pi.sample()
            logp_a = self.pi._log_prob_from_distribution(pi, a)
            v = self.v(obs)
        <span class="hljs-keyword">return</span> a.numpy(), v.numpy(), logp_a.numpy()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">return</span> self.step(obs)[<span class="hljs-number">0</span>]
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PPOBuffer</span>:</span>
    <span class="hljs-string">"""
    A buffer for storing trajectories experienced by a PPO agent interacting
    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)
    for calculating the advantages of state-action pairs.
    """</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, act_dim, size, gamma=<span class="hljs-number">0.99</span>, lam=<span class="hljs-number">0.95</span>)</span>:</span>
        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)
        self.adv_buf = np.zeros(size, dtype=np.float32)
        self.rew_buf = np.zeros(size, dtype=np.float32)
        self.ret_buf = np.zeros(size, dtype=np.float32)
        self.val_buf = np.zeros(size, dtype=np.float32)
        self.logp_buf = np.zeros(size, dtype=np.float32)
        self.gamma, self.lam = gamma, lam
        self.ptr, self.path_start_idx, self.max_size = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, size

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">store</span><span class="hljs-params">(self, obs, act, rew, val, logp)</span>:</span>
        <span class="hljs-string">"""
        Append one timestep of agent-environment interaction to the buffer.
        """</span>
        <span class="hljs-keyword">assert</span> self.ptr &lt; self.max_size     <span class="hljs-comment"># buffer has to have room so you can store</span>
        self.obs_buf[self.ptr] = obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.val_buf[self.ptr] = val
        self.logp_buf[self.ptr] = logp
        self.ptr += <span class="hljs-number">1</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish_path</span><span class="hljs-params">(self, last_val=<span class="hljs-number">0</span>)</span>:</span>
        <span class="hljs-string">"""
        Call this at the end of a trajectory, or when one gets cut off
        by an epoch ending. This looks back in the buffer to where the
        trajectory started, and uses rewards and value estimates from
        the whole trajectory to compute advantage estimates with GAE-Lambda,
        as well as compute the rewards-to-go for each state, to use as
        the targets for the value function.
        The "last_val" argument should be 0 if the trajectory ended
        because the agent reached a terminal state (died), and otherwise
        should be V(s_T), the value function estimated for the last state.
        This allows us to bootstrap the reward-to-go calculation to account
        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).
        """</span>

        path_slice = slice(self.path_start_idx, self.ptr)
        rews = np.append(self.rew_buf[path_slice], last_val)
        vals = np.append(self.val_buf[path_slice], last_val)
        
        <span class="hljs-comment"># the next two lines implement GAE-Lambda advantage calculation</span>
        deltas = rews[:<span class="hljs-number">-1</span>] + self.gamma * vals[<span class="hljs-number">1</span>:] - vals[:<span class="hljs-number">-1</span>]
        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)
        
        <span class="hljs-comment"># the next line computes rewards-to-go, to be targets for the value function</span>
        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:<span class="hljs-number">-1</span>]
        
        self.path_start_idx = self.ptr

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        Call this at the end of an epoch to get all of the data from
        the buffer, with advantages appropriately normalized (shifted to have
        mean zero and std one). Also, resets some pointers in the buffer.
        """</span>
        <span class="hljs-keyword">assert</span> self.ptr == self.max_size    <span class="hljs-comment"># buffer has to be full before you can get</span>
        self.ptr, self.path_start_idx = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
        <span class="hljs-comment"># the next two lines implement the advantage normalization trick</span>
        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / np.std(self.adv_buf)
        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,
                    adv=self.adv_buf, logp=self.logp_buf)
        <span class="hljs-keyword">return</span> {k: torch.as_tensor(v, dtype=torch.float32) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> data.items()}
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ppo</span><span class="hljs-params">(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict<span class="hljs-params">()</span>, seed=<span class="hljs-number">0</span>, 
        steps_per_epoch=<span class="hljs-number">4000</span>, epochs=<span class="hljs-number">50</span>, gamma=<span class="hljs-number">0.99</span>, clip_ratio=<span class="hljs-number">0.2</span>, pi_lr=<span class="hljs-number">3e-4</span>,
        vf_lr=<span class="hljs-number">1e-3</span>, train_pi_iters=<span class="hljs-number">80</span>, train_v_iters=<span class="hljs-number">80</span>, lam=<span class="hljs-number">0.97</span>, max_ep_len=<span class="hljs-number">1000</span>,
        target_kl=<span class="hljs-number">0.01</span>, save_freq=<span class="hljs-number">10</span>)</span>:</span>
    <span class="hljs-string">"""
    Proximal Policy Optimization (by clipping), 
    with early stopping based on approximate KL
    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.
        actor_critic: The constructor method for a PyTorch Module with a 
            ``step`` method, an ``act`` method, a ``pi`` module, and a ``v`` 
            module. The ``step`` method should accept a batch of observations 
            and return:
            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``a``        (batch, act_dim)  | Numpy array of actions for each 
                                           | observation.
            ``v``        (batch,)          | Numpy array of value estimates
                                           | for the provided observations.
            ``logp_a``   (batch,)          | Numpy array of log probs for the
                                           | actions in ``a``.
            ===========  ================  ======================================
            The ``act`` method behaves the same as ``step`` but only returns ``a``.
            The ``pi`` module's forward call should accept a batch of 
            observations and optionally a batch of actions, and return:
            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``pi``       N/A               | Torch Distribution object, containing
                                           | a batch of distributions describing
                                           | the policy for the provided observations.
            ``logp_a``   (batch,)          | Optional (only returned if batch of
                                           | actions is given). Tensor containing 
                                           | the log probability, according to 
                                           | the policy, of the provided actions.
                                           | If actions not given, will contain
                                           | ``None``.
            ===========  ================  ======================================
            The ``v`` module's forward call should accept a batch of observations
            and return:
            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``v``        (batch,)          | Tensor containing the value estimates
                                           | for the provided observations. (Critical: 
                                           | make sure to flatten this!)
            ===========  ================  ======================================
        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object 
            you provided to PPO.
        seed (int): Seed for random number generators.
        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.
        epochs (int): Number of epochs of interaction (equivalent to
            number of policy updates) to perform.
        gamma (float): Discount factor. (Always between 0 and 1.)
        clip_ratio (float): Hyperparameter for clipping in the policy objective.
            Roughly: how far can the new policy go from the old policy while 
            still profiting (improving the objective function)? The new policy 
            can still go farther than the clip_ratio says, but it doesn't help
            on the objective anymore. (Usually small, 0.1 to 0.3.) Typically
            denoted by :math:`\epsilon`. 
        pi_lr (float): Learning rate for policy optimizer.
        vf_lr (float): Learning rate for value function optimizer.
        train_pi_iters (int): Maximum number of gradient descent steps to take 
            on policy loss per epoch. (Early stopping may cause optimizer
            to take fewer than this.)
        train_v_iters (int): Number of gradient descent steps to take on 
            value function per epoch.
        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,
            close to 1.)
        max_ep_len (int): Maximum length of trajectory / episode / rollout.
        target_kl (float): Roughly what KL divergence we think is appropriate
            between new and old policies after an update. This will get used 
            for early stopping. (Usually small, 0.01 or 0.05.)
        logger_kwargs (dict): Keyword args for EpochLogger.
        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.
    """</span>

    <span class="hljs-comment"># Random seed</span>
    torch.manual_seed(seed)
    np.random.seed(seed)

    <span class="hljs-comment"># Instantiate environment</span>
    env = env_fn()
    obs_dim = env.observation_space.shape
    act_dim = env.action_space.shape

    <span class="hljs-comment"># Create actor-critic module</span>
    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)

    <span class="hljs-comment"># Count variables</span>
    var_counts = tuple(count_vars(module) <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> [ac.pi, ac.v])
    print(<span class="hljs-string">'\nNumber of parameters: \t pi: %d, \t v: %d\n'</span>%var_counts)

    <span class="hljs-comment"># Set up experience buffer</span>
    buf = PPOBuffer(obs_dim, act_dim, int(steps_per_epoch), gamma, lam)

    <span class="hljs-comment"># Set up function for computing PPO policy loss</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss_pi</span><span class="hljs-params">(data)</span>:</span>
        obs, act, adv, logp_old = data[<span class="hljs-string">'obs'</span>], data[<span class="hljs-string">'act'</span>], data[<span class="hljs-string">'adv'</span>], data[<span class="hljs-string">'logp'</span>]

        <span class="hljs-comment"># Policy loss</span>
        pi, logp = ac.pi(obs, act)
        ratio = torch.exp(logp - logp_old)
        clip_adv = torch.clamp(ratio, <span class="hljs-number">1</span>-clip_ratio, <span class="hljs-number">1</span>+clip_ratio) * adv
        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()

        <span class="hljs-comment"># Useful extra info</span>
        approx_kl = (logp_old - logp).mean().item()
        ent = pi.entropy().mean().item()
        clipped = ratio.gt(<span class="hljs-number">1</span>+clip_ratio) | ratio.lt(<span class="hljs-number">1</span>-clip_ratio)
        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()
        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)

        <span class="hljs-keyword">return</span> loss_pi, pi_info

    <span class="hljs-comment"># Set up function for computing value loss</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss_v</span><span class="hljs-params">(data)</span>:</span>
        obs, ret = data[<span class="hljs-string">'obs'</span>], data[<span class="hljs-string">'ret'</span>]
        <span class="hljs-keyword">return</span> ((ac.v(obs) - ret)**<span class="hljs-number">2</span>).mean()

    <span class="hljs-comment"># Set up optimizers for policy and value function</span>
    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)
    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">()</span>:</span>
        data = buf.get()

        pi_l_old, pi_info_old = compute_loss_pi(data)
        pi_l_old = pi_l_old.item()
        v_l_old = compute_loss_v(data).item()

        <span class="hljs-comment"># Train policy with multiple steps of gradient descent</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(train_pi_iters):
            pi_optimizer.zero_grad()
            loss_pi, pi_info = compute_loss_pi(data)
            
            <span class="hljs-keyword">if</span> pi_info[<span class="hljs-string">'kl'</span>] &gt; <span class="hljs-number">1.5</span> * target_kl:
                print(<span class="hljs-string">'Early stopping at step %d due to reaching max kl.'</span>%i)
                <span class="hljs-keyword">break</span>
            loss_pi.backward()
            pi_optimizer.step()

        <span class="hljs-comment"># Value function learning</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(train_v_iters):
            vf_optimizer.zero_grad()
            loss_v = compute_loss_v(data)
            loss_v.backward()
            vf_optimizer.step()

        <span class="hljs-comment"># Log changes from update</span>
        print(<span class="hljs-string">"DeltaLossPi={:.2f}, DeltaLossV={:.2f}"</span>.format(loss_pi.item() - pi_l_old, \
                                                             loss_v.item() - v_l_old))

    <span class="hljs-comment"># Prepare for interaction with environment</span>
    start_time = time.time()
    o, ep_ret, ep_len = env.reset(), <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
    
    average_ret = []
    <span class="hljs-comment"># Main loop: collect experience in env and update/log each epoch</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):
        check_point_ret = []
        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(steps_per_epoch):
            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))

            next_o, r, d, _ = env.step(a)
            ep_ret += r
            ep_len += <span class="hljs-number">1</span>

            <span class="hljs-comment"># save and log</span>
            buf.store(o, a, r, v, logp)
            
            <span class="hljs-comment"># Update obs (critical!)</span>
            o = next_o

            timeout = ep_len == max_ep_len
            terminal = d <span class="hljs-keyword">or</span> timeout
            epoch_ended = t==steps_per_epoch<span class="hljs-number">-1</span>

            <span class="hljs-keyword">if</span> terminal <span class="hljs-keyword">or</span> epoch_ended:
                <span class="hljs-keyword">if</span> epoch_ended <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span>(terminal):
                    print(<span class="hljs-string">'Warning: trajectory cut off by epoch at %d steps.'</span>%ep_len, flush=<span class="hljs-literal">True</span>)
                <span class="hljs-comment"># if trajectory didn't reach terminal state, bootstrap value target</span>
                <span class="hljs-keyword">if</span> timeout <span class="hljs-keyword">or</span> epoch_ended:
                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))
                <span class="hljs-keyword">else</span>:
                    v = <span class="hljs-number">0</span>
                buf.finish_path(v)
                <span class="hljs-keyword">if</span> terminal:
                    <span class="hljs-comment"># only save EpRet / EpLen if trajectory finished</span>
                    print(<span class="hljs-string">"Episodic Ret={:.2f}, Episodic Len={:.2f}"</span>.format(ep_ret, ep_len))
                    check_point_ret.append(ep_ret)
                o, ep_ret, ep_len = env.reset(), <span class="hljs-number">0</span>, <span class="hljs-number">0</span>

        
        print(<span class="hljs-string">"Epoch: {}"</span>.format(epoch))
        <span class="hljs-comment"># record improvement</span>
        average_ret.append(np.mean(check_point_ret))

        <span class="hljs-comment"># Perform PPO update!</span>
        update()
        
    <span class="hljs-keyword">return</span> average_ret
</div></code></pre>
<pre><code>&lt;&gt;:5: DeprecationWarning: invalid escape sequence \e
</code></pre>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> argparse
parser = argparse.ArgumentParser()
parser.add_argument(<span class="hljs-string">'--env'</span>, type=str, default=<span class="hljs-string">"BipedalWalker-v3"</span>)
<span class="hljs-comment"># To use to the hardcore environment, you need to specify the hardcore=True argument</span>
<span class="hljs-comment"># env = gym.make("BipedalWalker-v3", hardcore=True)</span>
parser.add_argument(<span class="hljs-string">'--hid'</span>, type=int, default=<span class="hljs-number">64</span>)
parser.add_argument(<span class="hljs-string">'--l'</span>, type=int, default=<span class="hljs-number">2</span>)
parser.add_argument(<span class="hljs-string">'--gamma'</span>, type=float, default=<span class="hljs-number">0.99</span>)
parser.add_argument(<span class="hljs-string">'--seed'</span>, <span class="hljs-string">'-s'</span>, type=int, default=<span class="hljs-number">0</span>)
parser.add_argument(<span class="hljs-string">'--steps'</span>, type=int, default=<span class="hljs-number">4000</span>)
parser.add_argument(<span class="hljs-string">'--epochs'</span>, type=int, default=<span class="hljs-number">50</span>)
args, unknown = parser.parse_known_args()


average_ret = ppo(<span class="hljs-keyword">lambda</span> : gym.make(args.env), actor_critic=MLPActorCritic,
    ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, 
    seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,)
</div></code></pre>
<pre><code>Number of parameters: 	 pi: 6024, 	 v: 5825

Episodic Ret=-49.82, Episodic Len=1000.00
Episodic Ret=-49.46, Episodic Len=1000.00
Episodic Ret=-108.25, Episodic Len=74.00
Episodic Ret=-97.25, Episodic Len=88.00
Episodic Ret=-122.20, Episodic Len=127.00
Episodic Ret=-118.35, Episodic Len=60.00
Episodic Ret=-116.01, Episodic Len=85.00
Episodic Ret=-112.22, Episodic Len=70.00
Episodic Ret=-46.72, Episodic Len=1000.00
Episodic Ret=-102.36, Episodic Len=54.00
Episodic Ret=-120.68, Episodic Len=75.00
Warning: trajectory cut off by epoch at 367 steps.
Epoch: 0
DeltaLossPi=-0.01, DeltaLossV=-193.73
Episodic Ret=-117.93, Episodic Len=68.00
Episodic Ret=-118.57, Episodic Len=71.00
Episodic Ret=-101.59, Episodic Len=74.00
Episodic Ret=-47.25, Episodic Len=1000.00
Episodic Ret=-46.59, Episodic Len=1000.00
Episodic Ret=-108.04, Episodic Len=45.00
Episodic Ret=-53.12, Episodic Len=1000.00
Episodic Ret=-118.99, Episodic Len=135.00
Warning: trajectory cut off by epoch at 607 steps.
Epoch: 1
DeltaLossPi=-0.02, DeltaLossV=-41.39
Episodic Ret=-104.48, Episodic Len=80.00
Episodic Ret=-113.39, Episodic Len=60.00
Episodic Ret=-48.46, Episodic Len=1000.00
Episodic Ret=-109.15, Episodic Len=131.00
Episodic Ret=-50.44, Episodic Len=1000.00
Episodic Ret=-101.05, Episodic Len=112.00
Episodic Ret=-53.58, Episodic Len=1000.00
Episodic Ret=-108.71, Episodic Len=50.00
Warning: trajectory cut off by epoch at 567 steps.
Epoch: 2
DeltaLossPi=-0.02, DeltaLossV=-56.66
Episodic Ret=-52.94, Episodic Len=1000.00
Episodic Ret=-98.16, Episodic Len=102.00
Episodic Ret=-51.59, Episodic Len=1000.00
Episodic Ret=-102.22, Episodic Len=73.00
Episodic Ret=-106.17, Episodic Len=83.00
Episodic Ret=-51.13, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 742 steps.
Epoch: 3
DeltaLossPi=-0.01, DeltaLossV=-24.90
Episodic Ret=-48.40, Episodic Len=1000.00
Episodic Ret=-101.94, Episodic Len=86.00
Episodic Ret=-50.62, Episodic Len=1000.00
Episodic Ret=-51.49, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 914 steps.
Epoch: 4
DeltaLossPi=-0.01, DeltaLossV=-5.88
Episodic Ret=-100.28, Episodic Len=70.00
Episodic Ret=-102.39, Episodic Len=60.00
Episodic Ret=-51.18, Episodic Len=1000.00
Episodic Ret=-55.31, Episodic Len=1000.00
Episodic Ret=-101.26, Episodic Len=53.00
Episodic Ret=-98.00, Episodic Len=77.00
Episodic Ret=-51.36, Episodic Len=1000.00
Episodic Ret=-104.15, Episodic Len=75.00
Warning: trajectory cut off by epoch at 665 steps.
Epoch: 5
DeltaLossPi=-0.02, DeltaLossV=-48.31
Episodic Ret=-54.39, Episodic Len=1000.00
Episodic Ret=-117.54, Episodic Len=83.00
Episodic Ret=-50.19, Episodic Len=1000.00
Episodic Ret=-100.69, Episodic Len=83.00
Episodic Ret=-99.49, Episodic Len=114.00
Episodic Ret=-55.70, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 720 steps.
Epoch: 6
DeltaLossPi=-0.02, DeltaLossV=-40.69
Episodic Ret=-58.54, Episodic Len=1000.00
Episodic Ret=-57.52, Episodic Len=1000.00
Episodic Ret=-58.34, Episodic Len=1000.00
Episodic Ret=-52.94, Episodic Len=1000.00
Epoch: 7
DeltaLossPi=-0.02, DeltaLossV=-6.33
Episodic Ret=-50.19, Episodic Len=1000.00
Episodic Ret=-49.88, Episodic Len=1000.00
Episodic Ret=-56.56, Episodic Len=1000.00
Episodic Ret=-51.16, Episodic Len=1000.00
Epoch: 8
Early stopping at step 5 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-1.94
Episodic Ret=-57.32, Episodic Len=1000.00
Episodic Ret=-116.96, Episodic Len=72.00
Episodic Ret=-106.12, Episodic Len=76.00
Episodic Ret=-48.67, Episodic Len=1000.00
Episodic Ret=-45.16, Episodic Len=1000.00
Episodic Ret=-119.71, Episodic Len=101.00
Warning: trajectory cut off by epoch at 751 steps.
Epoch: 9
Early stopping at step 36 due to reaching max kl.
DeltaLossPi=-0.02, DeltaLossV=-132.63
Episodic Ret=-44.97, Episodic Len=1000.00
Episodic Ret=-114.19, Episodic Len=69.00
Episodic Ret=-48.86, Episodic Len=1000.00
Episodic Ret=-45.78, Episodic Len=1000.00
Episodic Ret=-105.31, Episodic Len=105.00
Episodic Ret=-97.61, Episodic Len=108.00
Episodic Ret=-108.80, Episodic Len=54.00
Warning: trajectory cut off by epoch at 664 steps.
Epoch: 10
Early stopping at step 7 due to reaching max kl.
DeltaLossPi=-0.01, DeltaLossV=-82.88
Episodic Ret=-55.30, Episodic Len=1000.00
Episodic Ret=-44.32, Episodic Len=1000.00
Episodic Ret=-42.77, Episodic Len=1000.00
Episodic Ret=-38.79, Episodic Len=1000.00
Epoch: 11
Early stopping at step 17 due to reaching max kl.
DeltaLossPi=-0.01, DeltaLossV=-15.30
Episodic Ret=-50.35, Episodic Len=1000.00
Episodic Ret=-43.89, Episodic Len=1000.00
Episodic Ret=-43.60, Episodic Len=1000.00
Episodic Ret=-43.42, Episodic Len=1000.00
Epoch: 12
DeltaLossPi=-0.01, DeltaLossV=-0.56
Episodic Ret=-51.58, Episodic Len=1000.00
Episodic Ret=-45.65, Episodic Len=1000.00
Episodic Ret=-47.01, Episodic Len=1000.00
Episodic Ret=-47.63, Episodic Len=1000.00
Epoch: 13
DeltaLossPi=-0.01, DeltaLossV=-0.06
Episodic Ret=-48.77, Episodic Len=1000.00
Episodic Ret=-48.36, Episodic Len=1000.00
Episodic Ret=-43.41, Episodic Len=1000.00
Episodic Ret=-43.06, Episodic Len=1000.00
Epoch: 14
DeltaLossPi=-0.01, DeltaLossV=-0.10
Episodic Ret=-44.81, Episodic Len=1000.00
Episodic Ret=-46.16, Episodic Len=1000.00
Episodic Ret=-42.71, Episodic Len=1000.00
Episodic Ret=-40.21, Episodic Len=1000.00
Epoch: 15
DeltaLossPi=-0.02, DeltaLossV=-0.09
Episodic Ret=-110.32, Episodic Len=50.00
Episodic Ret=-41.16, Episodic Len=1000.00
Episodic Ret=-42.81, Episodic Len=1000.00
Episodic Ret=-34.33, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 950 steps.
Epoch: 16
Early stopping at step 76 due to reaching max kl.
DeltaLossPi=-0.01, DeltaLossV=-24.37
Episodic Ret=-43.80, Episodic Len=1000.00
Episodic Ret=-39.41, Episodic Len=1000.00
Episodic Ret=-116.43, Episodic Len=67.00
Episodic Ret=-39.31, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 933 steps.
Epoch: 17
DeltaLossPi=-0.01, DeltaLossV=-36.21
Episodic Ret=-110.64, Episodic Len=86.00
Episodic Ret=-108.98, Episodic Len=67.00
Episodic Ret=-39.49, Episodic Len=1000.00
Episodic Ret=-43.95, Episodic Len=1000.00
Episodic Ret=-28.40, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 847 steps.
Epoch: 18
DeltaLossPi=-0.02, DeltaLossV=-25.09
Episodic Ret=-43.93, Episodic Len=1000.00
Episodic Ret=-118.91, Episodic Len=61.00
Episodic Ret=-43.10, Episodic Len=1000.00
Episodic Ret=-40.76, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 939 steps.
Epoch: 19
DeltaLossPi=-0.01, DeltaLossV=-13.67
Episodic Ret=-44.76, Episodic Len=1000.00
Episodic Ret=-47.49, Episodic Len=1000.00
Episodic Ret=-39.61, Episodic Len=1000.00
Episodic Ret=-43.12, Episodic Len=1000.00
Epoch: 20
DeltaLossPi=-0.01, DeltaLossV=-5.71
Episodic Ret=-38.43, Episodic Len=1000.00
Episodic Ret=-43.32, Episodic Len=1000.00
Episodic Ret=-46.32, Episodic Len=1000.00
Episodic Ret=-41.36, Episodic Len=1000.00
Epoch: 21
Early stopping at step 6 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-3.59
Episodic Ret=-35.10, Episodic Len=1000.00
Episodic Ret=-42.23, Episodic Len=1000.00
Episodic Ret=-35.52, Episodic Len=1000.00
Episodic Ret=-29.74, Episodic Len=1000.00
Epoch: 22
DeltaLossPi=-0.02, DeltaLossV=-0.52
Episodic Ret=-37.97, Episodic Len=1000.00
Episodic Ret=-39.05, Episodic Len=1000.00
Episodic Ret=-33.42, Episodic Len=1000.00
Episodic Ret=-36.29, Episodic Len=1000.00
Epoch: 23
DeltaLossPi=-0.02, DeltaLossV=-0.04
Episodic Ret=-39.74, Episodic Len=1000.00
Episodic Ret=-33.37, Episodic Len=1000.00
Episodic Ret=-38.34, Episodic Len=1000.00
Episodic Ret=-31.71, Episodic Len=1000.00
Epoch: 24
Early stopping at step 4 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-0.05
Episodic Ret=-32.43, Episodic Len=1000.00
Episodic Ret=-34.62, Episodic Len=1000.00
Episodic Ret=-35.03, Episodic Len=1000.00
Episodic Ret=-31.66, Episodic Len=1000.00
Epoch: 25
Early stopping at step 3 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-0.03
Episodic Ret=-30.58, Episodic Len=1000.00
Episodic Ret=-32.26, Episodic Len=1000.00
Episodic Ret=-33.94, Episodic Len=1000.00
Episodic Ret=-30.55, Episodic Len=1000.00
Epoch: 26
DeltaLossPi=-0.01, DeltaLossV=-0.10
Episodic Ret=-36.84, Episodic Len=1000.00
Episodic Ret=-28.47, Episodic Len=1000.00
Episodic Ret=-33.80, Episodic Len=1000.00
Episodic Ret=-26.20, Episodic Len=1000.00
Epoch: 27
DeltaLossPi=-0.02, DeltaLossV=-0.07
Episodic Ret=-30.88, Episodic Len=1000.00
Episodic Ret=-29.36, Episodic Len=1000.00
Episodic Ret=-24.18, Episodic Len=1000.00
Episodic Ret=-29.44, Episodic Len=1000.00
Epoch: 28
DeltaLossPi=-0.02, DeltaLossV=-0.04
Episodic Ret=-24.69, Episodic Len=1000.00
Episodic Ret=-31.80, Episodic Len=1000.00
Episodic Ret=-19.87, Episodic Len=1000.00
Episodic Ret=-31.40, Episodic Len=1000.00
Epoch: 29
DeltaLossPi=-0.02, DeltaLossV=-0.12
Episodic Ret=-13.44, Episodic Len=1000.00
Episodic Ret=-17.44, Episodic Len=1000.00
Episodic Ret=-31.43, Episodic Len=1000.00
Episodic Ret=-28.74, Episodic Len=1000.00
Epoch: 30
DeltaLossPi=-0.02, DeltaLossV=-0.20
Episodic Ret=-19.28, Episodic Len=1000.00
Episodic Ret=-23.64, Episodic Len=1000.00
Episodic Ret=-12.91, Episodic Len=1000.00
Episodic Ret=-18.36, Episodic Len=1000.00
Epoch: 31
DeltaLossPi=-0.02, DeltaLossV=-0.19
Episodic Ret=-14.75, Episodic Len=1000.00
Episodic Ret=-22.35, Episodic Len=1000.00
Episodic Ret=-23.69, Episodic Len=1000.00
Episodic Ret=-10.84, Episodic Len=1000.00
Epoch: 32
DeltaLossPi=-0.02, DeltaLossV=-0.36
Episodic Ret=-20.05, Episodic Len=1000.00
Episodic Ret=-9.15, Episodic Len=1000.00
Episodic Ret=-13.02, Episodic Len=1000.00
Episodic Ret=-15.69, Episodic Len=1000.00
Epoch: 33
DeltaLossPi=-0.02, DeltaLossV=-0.11
Episodic Ret=-10.58, Episodic Len=1000.00
Episodic Ret=-8.50, Episodic Len=1000.00
Episodic Ret=-2.78, Episodic Len=1000.00
Episodic Ret=-17.64, Episodic Len=1000.00
Epoch: 34
DeltaLossPi=-0.02, DeltaLossV=-0.51
Episodic Ret=-109.78, Episodic Len=59.00
Episodic Ret=-3.25, Episodic Len=1000.00
Episodic Ret=-2.39, Episodic Len=1000.00
Episodic Ret=-111.19, Episodic Len=55.00
Episodic Ret=-12.19, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 886 steps.
Epoch: 35
Early stopping at step 5 due to reaching max kl.
DeltaLossPi=-0.01, DeltaLossV=-54.64
Episodic Ret=-12.67, Episodic Len=1000.00
Episodic Ret=1.38, Episodic Len=1000.00
Episodic Ret=-107.08, Episodic Len=693.00
Episodic Ret=-4.18, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 307 steps.
Epoch: 36
DeltaLossPi=-0.02, DeltaLossV=-20.00
Episodic Ret=-7.88, Episodic Len=1000.00
Episodic Ret=-2.70, Episodic Len=1000.00
Episodic Ret=-2.82, Episodic Len=1000.00
Episodic Ret=-13.12, Episodic Len=1000.00
Epoch: 37
Early stopping at step 4 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-3.47
Episodic Ret=-9.17, Episodic Len=1000.00
Episodic Ret=-6.67, Episodic Len=1000.00
Episodic Ret=-5.34, Episodic Len=1000.00
Episodic Ret=2.00, Episodic Len=1000.00
Epoch: 38
DeltaLossPi=-0.02, DeltaLossV=-0.20
Episodic Ret=-8.06, Episodic Len=1000.00
Episodic Ret=-2.59, Episodic Len=1000.00
Episodic Ret=9.68, Episodic Len=1000.00
Episodic Ret=-9.98, Episodic Len=1000.00
Epoch: 39
DeltaLossPi=-0.01, DeltaLossV=-0.15
Episodic Ret=-3.68, Episodic Len=1000.00
Episodic Ret=1.29, Episodic Len=1000.00
Episodic Ret=0.01, Episodic Len=1000.00
Episodic Ret=-4.43, Episodic Len=1000.00
Epoch: 40
DeltaLossPi=-0.01, DeltaLossV=-0.15
Episodic Ret=1.65, Episodic Len=1000.00
Episodic Ret=2.54, Episodic Len=1000.00
Episodic Ret=7.52, Episodic Len=1000.00
Episodic Ret=-99.74, Episodic Len=79.00
Warning: trajectory cut off by epoch at 921 steps.
Epoch: 41
DeltaLossPi=-0.02, DeltaLossV=-34.52
Episodic Ret=3.68, Episodic Len=1000.00
Episodic Ret=5.80, Episodic Len=1000.00
Episodic Ret=1.26, Episodic Len=1000.00
Episodic Ret=0.08, Episodic Len=1000.00
Epoch: 42
Early stopping at step 4 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-0.48
Episodic Ret=2.63, Episodic Len=1000.00
Episodic Ret=19.94, Episodic Len=1000.00
Episodic Ret=12.05, Episodic Len=1000.00
Episodic Ret=1.10, Episodic Len=1000.00
Epoch: 43
DeltaLossPi=-0.01, DeltaLossV=-0.78
Episodic Ret=-4.68, Episodic Len=1000.00
Episodic Ret=5.37, Episodic Len=1000.00
Episodic Ret=13.18, Episodic Len=1000.00
Episodic Ret=-117.46, Episodic Len=85.00
Warning: trajectory cut off by epoch at 915 steps.
Epoch: 44
Early stopping at step 13 due to reaching max kl.
DeltaLossPi=-0.01, DeltaLossV=-4.11
Episodic Ret=-118.21, Episodic Len=97.00
Episodic Ret=-4.89, Episodic Len=1000.00
Episodic Ret=13.13, Episodic Len=1000.00
Episodic Ret=4.10, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 903 steps.
Epoch: 45
Early stopping at step 5 due to reaching max kl.
DeltaLossPi=-0.00, DeltaLossV=-6.07
Episodic Ret=9.50, Episodic Len=1000.00
Episodic Ret=-110.43, Episodic Len=56.00
Episodic Ret=20.94, Episodic Len=1000.00
Episodic Ret=12.17, Episodic Len=1000.00
Warning: trajectory cut off by epoch at 944 steps.
Epoch: 46
DeltaLossPi=-0.02, DeltaLossV=-8.06
Episodic Ret=0.36, Episodic Len=1000.00
Episodic Ret=5.83, Episodic Len=1000.00
Episodic Ret=10.09, Episodic Len=1000.00
Episodic Ret=-2.32, Episodic Len=1000.00
Epoch: 47
DeltaLossPi=-0.02, DeltaLossV=-0.59
Episodic Ret=23.15, Episodic Len=1000.00
Episodic Ret=12.53, Episodic Len=1000.00
Episodic Ret=39.45, Episodic Len=1000.00
Episodic Ret=6.06, Episodic Len=1000.00
Epoch: 48
DeltaLossPi=-0.01, DeltaLossV=-1.86
Episodic Ret=27.47, Episodic Len=1000.00
Episodic Ret=36.24, Episodic Len=1000.00
Episodic Ret=17.64, Episodic Len=1000.00
Episodic Ret=-98.18, Episodic Len=135.00
Warning: trajectory cut off by epoch at 865 steps.
Epoch: 49
Early stopping at step 15 due to reaching max kl.
DeltaLossPi=-0.01, DeltaLossV=-45.70
</code></pre>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

plt.figure()
plt.plot(average_ret)
plt.xlabel(<span class="hljs-string">'Training Epoch'</span>)
plt.ylabel(<span class="hljs-string">'Average Return'</span>)
plt.show()
</div></code></pre>
<p><img src="output_7_0.png" alt="png"></p>

</body>
</html>
