<!DOCTYPE html>
<html>
<head>
<title>06_Vanilla Policy Gradient for Mountain Car Continuous Control.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="mountain-car-continuous-control-with-vpg">Mountain Car Continuous Control with VPG</h1>
<div>
<img src=https://gymnasium.farama.org/_images/mountain_car.gif width=40%/>
</div>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> scipy.signal
<span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> Box, Discrete

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">from</span> torch.distributions.normal <span class="hljs-keyword">import</span> Normal
<span class="hljs-keyword">from</span> torch.distributions.categorical <span class="hljs-keyword">import</span> Categorical
<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam

<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> time
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">combined_shape</span><span class="hljs-params">(length, shape=None)</span>:</span>
    <span class="hljs-keyword">if</span> shape <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> (length,)
    <span class="hljs-keyword">return</span> (length, shape) <span class="hljs-keyword">if</span> np.isscalar(shape) <span class="hljs-keyword">else</span> (length, *shape)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mlp</span><span class="hljs-params">(sizes, activation, output_activation=nn.Identity)</span>:</span>
    layers = []
    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(sizes)<span class="hljs-number">-1</span>):
        act = activation <span class="hljs-keyword">if</span> j &lt; len(sizes)<span class="hljs-number">-2</span> <span class="hljs-keyword">else</span> output_activation
        layers += [nn.Linear(sizes[j], sizes[j+<span class="hljs-number">1</span>]), act()]
    <span class="hljs-keyword">return</span> nn.Sequential(*layers)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">count_vars</span><span class="hljs-params">(module)</span>:</span>
    <span class="hljs-keyword">return</span> sum([np.prod(p.shape) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> module.parameters()])


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discount_cumsum</span><span class="hljs-params">(x, discount)</span>:</span>
    <span class="hljs-string">"""
    magic from rllab for computing discounted cumulative sums of vectors.
    input: 
        vector x, 
        [x0, 
         x1, 
         x2]
    output:
        [x0 + discount * x1 + discount^2 * x2,  
         x1 + discount * x2,
         x2]
    """</span>
    <span class="hljs-keyword">return</span> scipy.signal.lfilter([<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, float(-discount)], x[::<span class="hljs-number">-1</span>], axis=<span class="hljs-number">0</span>)[::<span class="hljs-number">-1</span>]


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actor</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_distribution</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">raise</span> NotImplementedError

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_log_prob_from_distribution</span><span class="hljs-params">(self, pi, act)</span>:</span>
        <span class="hljs-keyword">raise</span> NotImplementedError

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, obs, act=None)</span>:</span>
        <span class="hljs-comment"># Produce action distributions for given observations, and </span>
        <span class="hljs-comment"># optionally compute the log likelihood of given actions under</span>
        <span class="hljs-comment"># those distributions.</span>
        pi = self._distribution(obs)
        logp_a = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">if</span> act <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            logp_a = self._log_prob_from_distribution(pi, act)
        <span class="hljs-keyword">return</span> pi, logp_a


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPCategoricalActor</span><span class="hljs-params">(Actor)</span>:</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, act_dim, hidden_sizes, activation)</span>:</span>
        super().__init__()
        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_distribution</span><span class="hljs-params">(self, obs)</span>:</span>
        logits = self.logits_net(obs)
        <span class="hljs-keyword">return</span> Categorical(logits=logits)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_log_prob_from_distribution</span><span class="hljs-params">(self, pi, act)</span>:</span>
        <span class="hljs-keyword">return</span> pi.log_prob(act)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPGaussianActor</span><span class="hljs-params">(Actor)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, act_dim, hidden_sizes, activation)</span>:</span>
        super().__init__()
        log_std = <span class="hljs-number">-0.5</span> * np.ones(act_dim, dtype=np.float32)
        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))
        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_distribution</span><span class="hljs-params">(self, obs)</span>:</span>
        mu = self.mu_net(obs)
        std = torch.exp(self.log_std)
        <span class="hljs-keyword">return</span> Normal(mu, std)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_log_prob_from_distribution</span><span class="hljs-params">(self, pi, act)</span>:</span>
        <span class="hljs-keyword">return</span> pi.log_prob(act).sum(axis=<span class="hljs-number">-1</span>)    <span class="hljs-comment"># Last axis sum needed for Torch Normal distribution</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPCritic</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, hidden_sizes, activation)</span>:</span>
        super().__init__()
        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [<span class="hljs-number">1</span>], activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">return</span> torch.squeeze(self.v_net(obs), <span class="hljs-number">-1</span>) <span class="hljs-comment"># Critical to ensure v has right shape.</span>



<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPActorCritic</span><span class="hljs-params">(nn.Module)</span>:</span>


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, observation_space, action_space, 
                 hidden_sizes=<span class="hljs-params">(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>)</span>, activation=nn.Tanh)</span>:</span>
        super().__init__()

        obs_dim = observation_space.shape[<span class="hljs-number">0</span>]

        <span class="hljs-comment"># policy builder depends on action space</span>
        <span class="hljs-keyword">if</span> isinstance(action_space, Box):
            self.pi = MLPGaussianActor(obs_dim, action_space.shape[<span class="hljs-number">0</span>], hidden_sizes, activation)
        <span class="hljs-keyword">elif</span> isinstance(action_space, Discrete):
            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)

        <span class="hljs-comment"># build value function</span>
        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">with</span> torch.no_grad():
            pi = self.pi._distribution(obs)
            a = pi.sample()
            logp_a = self.pi._log_prob_from_distribution(pi, a)
            v = self.v(obs)
        <span class="hljs-keyword">return</span> a.numpy(), v.numpy(), logp_a.numpy()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span><span class="hljs-params">(self, obs)</span>:</span>
        <span class="hljs-keyword">return</span> self.step(obs)[<span class="hljs-number">0</span>]
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VPGBuffer</span>:</span>
    <span class="hljs-string">"""
    A buffer for storing trajectories experienced by a VPG agent interacting
    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)
    for calculating the advantages of state-action pairs.
    """</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, obs_dim, act_dim, size, gamma=<span class="hljs-number">0.99</span>, lam=<span class="hljs-number">0.95</span>)</span>:</span>
        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)
        self.adv_buf = np.zeros(size, dtype=np.float32)
        self.rew_buf = np.zeros(size, dtype=np.float32)
        self.ret_buf = np.zeros(size, dtype=np.float32)
        self.val_buf = np.zeros(size, dtype=np.float32)
        self.logp_buf = np.zeros(size, dtype=np.float32)
        self.gamma, self.lam = gamma, lam
        self.ptr, self.path_start_idx, self.max_size = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, size

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">store</span><span class="hljs-params">(self, obs, act, rew, val, logp)</span>:</span>
        <span class="hljs-string">"""
        Append one timestep of agent-environment interaction to the buffer.
        """</span>
        <span class="hljs-keyword">assert</span> self.ptr &lt; self.max_size     <span class="hljs-comment"># buffer has to have room so you can store</span>
        self.obs_buf[self.ptr] = obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.val_buf[self.ptr] = val
        self.logp_buf[self.ptr] = logp
        self.ptr += <span class="hljs-number">1</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish_path</span><span class="hljs-params">(self, last_val=<span class="hljs-number">0</span>)</span>:</span>
        <span class="hljs-string">"""
        Call this at the end of a trajectory, or when one gets cut off
        by an epoch ending. This looks back in the buffer to where the
        trajectory started, and uses rewards and value estimates from
        the whole trajectory to compute advantage estimates with GAE-Lambda,
        as well as compute the rewards-to-go for each state, to use as
        the targets for the value function.
        The "last_val" argument should be 0 if the trajectory ended
        because the agent reached a terminal state (died), and otherwise
        should be V(s_T), the value function estimated for the last state.
        This allows us to bootstrap the reward-to-go calculation to account
        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).
        """</span>

        path_slice = slice(self.path_start_idx, self.ptr)
        rews = np.append(self.rew_buf[path_slice], last_val)
        vals = np.append(self.val_buf[path_slice], last_val)
        
        <span class="hljs-comment"># the next two lines implement GAE-Lambda advantage calculation</span>
        deltas = rews[:<span class="hljs-number">-1</span>] + self.gamma * vals[<span class="hljs-number">1</span>:] - vals[:<span class="hljs-number">-1</span>]
        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)
        
        <span class="hljs-comment"># the next line computes rewards-to-go, to be targets for the value function</span>
        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:<span class="hljs-number">-1</span>]
        
        self.path_start_idx = self.ptr

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        Call this at the end of an epoch to get all of the data from
        the buffer, with advantages appropriately normalized (shifted to have
        mean zero and std one). Also, resets some pointers in the buffer.
        """</span>
        <span class="hljs-keyword">assert</span> self.ptr == self.max_size    <span class="hljs-comment"># buffer has to be full before you can get</span>
        self.ptr, self.path_start_idx = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
        <span class="hljs-comment"># the next line implement the advantage normalization trick</span>
        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / np.std(self.adv_buf)
        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,
                    adv=self.adv_buf, logp=self.logp_buf)
        <span class="hljs-keyword">return</span> {k: torch.as_tensor(v, dtype=torch.float32) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> data.items()}



<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vpg</span><span class="hljs-params">(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict<span class="hljs-params">()</span>,  seed=<span class="hljs-number">0</span>, 
        steps_per_epoch=<span class="hljs-number">4000</span>, epochs=<span class="hljs-number">50</span>, gamma=<span class="hljs-number">0.99</span>, pi_lr=<span class="hljs-number">3e-4</span>,
        vf_lr=<span class="hljs-number">1e-3</span>, train_v_iters=<span class="hljs-number">80</span>, lam=<span class="hljs-number">0.97</span>, max_ep_len=<span class="hljs-number">1000</span>,
        logger_kwargs=dict<span class="hljs-params">()</span>, save_freq=<span class="hljs-number">10</span>)</span>:</span>
    <span class="hljs-string">"""
    Vanilla Policy Gradient 
    (with GAE-Lambda for advantage estimation)
    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.
        actor_critic: The constructor method for a PyTorch Module with a 
            ``step`` method, an ``act`` method, a ``pi`` module, and a ``v`` 
            module. The ``step`` method should accept a batch of observations 
            and return:
            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``a``        (batch, act_dim)  | Numpy array of actions for each 
                                           | observation.
            ``v``        (batch,)          | Numpy array of value estimates
                                           | for the provided observations.
            ``logp_a``   (batch,)          | Numpy array of log probs for the
                                           | actions in ``a``.
            ===========  ================  ======================================
            The ``act`` method behaves the same as ``step`` but only returns ``a``.
            The ``pi`` module's forward call should accept a batch of 
            observations and optionally a batch of actions, and return:
            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``pi``       N/A               | Torch Distribution object, containing
                                           | a batch of distributions describing
                                           | the policy for the provided observations.
            ``logp_a``   (batch,)          | Optional (only returned if batch of
                                           | actions is given). Tensor containing 
                                           | the log probability, according to 
                                           | the policy, of the provided actions.
                                           | If actions not given, will contain
                                           | ``None``.
            ===========  ================  ======================================
            The ``v`` module's forward call should accept a batch of observations
            and return:
            ===========  ================  ======================================
            Symbol       Shape             Description
            ===========  ================  ======================================
            ``v``        (batch,)          | Tensor containing the value estimates
                                           | for the provided observations. (Critical: 
                                           | make sure to flatten this!)
            ===========  ================  ======================================
        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object 
            you provided to VPG.
        seed (int): Seed for random number generators.
        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.
        epochs (int): Number of epochs of interaction (equivalent to
            number of policy updates) to perform.
        gamma (float): Discount factor. (Always between 0 and 1.)
        pi_lr (float): Learning rate for policy optimizer.
        vf_lr (float): Learning rate for value function optimizer.
        train_v_iters (int): Number of gradient descent steps to take on 
            value function per epoch.
        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,
            close to 1.)
        max_ep_len (int): Maximum length of trajectory / episode / rollout.
        logger_kwargs (dict): Keyword args for EpochLogger.
        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.
    """</span>

    <span class="hljs-comment"># Random seed</span>
    torch.manual_seed(seed)
    np.random.seed(seed)

    <span class="hljs-comment"># Instantiate environment</span>
    env = env_fn()
    obs_dim = env.observation_space.shape
    act_dim = env.action_space.shape

    <span class="hljs-comment"># Create actor-critic module</span>
    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)

    <span class="hljs-comment"># Count variables</span>
    var_counts = tuple(count_vars(module) <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> [ac.pi, ac.v])
    print(<span class="hljs-string">'\nNumber of parameters: \t pi: %d, \t v: %d\n'</span>%var_counts)

    <span class="hljs-comment"># Set up experience buffer</span>
    local_steps_per_epoch = int(steps_per_epoch)
    buf = VPGBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)

    <span class="hljs-comment"># Set up function for computing VPG policy loss</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss_pi</span><span class="hljs-params">(data)</span>:</span>
        obs, act, adv, logp_old = data[<span class="hljs-string">'obs'</span>], data[<span class="hljs-string">'act'</span>], data[<span class="hljs-string">'adv'</span>], data[<span class="hljs-string">'logp'</span>]

        <span class="hljs-comment"># Policy loss</span>
        pi, logp = ac.pi(obs, act)
        loss_pi = -(logp * adv).mean()

        <span class="hljs-comment"># Useful extra info</span>
        approx_kl = (logp_old - logp).mean().item()
        ent = pi.entropy().mean().item()
        pi_info = dict(kl=approx_kl, ent=ent)

        <span class="hljs-keyword">return</span> loss_pi, pi_info

    <span class="hljs-comment"># Set up function for computing value loss</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss_v</span><span class="hljs-params">(data)</span>:</span>
        obs, ret = data[<span class="hljs-string">'obs'</span>], data[<span class="hljs-string">'ret'</span>]
        <span class="hljs-keyword">return</span> ((ac.v(obs) - ret)**<span class="hljs-number">2</span>).mean()

    <span class="hljs-comment"># Set up optimizers for policy and value function</span>
    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)
    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">()</span>:</span>
        data = buf.get()

        <span class="hljs-comment"># Train policy with a single step of gradient descent</span>
        pi_optimizer.zero_grad()
        loss_pi, pi_info = compute_loss_pi(data)
        loss_pi.backward()
        pi_optimizer.step()

        <span class="hljs-comment"># Value function learning</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(train_v_iters):
            vf_optimizer.zero_grad()
            loss_v = compute_loss_v(data)
            loss_v.backward()
            vf_optimizer.step()
        
        print(<span class="hljs-string">'LossPi: {:.4f}, LossV: {:.4f}'</span>.format(loss_pi.item(), loss_v.item()))

    <span class="hljs-comment"># Prepare for interaction with environment</span>
    start_time = time.time()
    o, ep_ret, ep_len = env.reset(), <span class="hljs-number">0</span>, <span class="hljs-number">0</span>

    <span class="hljs-comment"># Main loop: collect experience in env and update/log each epoch</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):
        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(local_steps_per_epoch):
            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))
            a = np.clip(a, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)
            next_o, r, d, _ = env.step(a)
            ep_ret += r
            ep_len += <span class="hljs-number">1</span>

            <span class="hljs-comment"># save and log</span>
            buf.store(o, a, r, v, logp)
            
            <span class="hljs-comment"># Update obs (critical!)</span>
            o = next_o

            timeout = ep_len == max_ep_len
            terminal = d <span class="hljs-keyword">or</span> timeout
            epoch_ended = t==local_steps_per_epoch<span class="hljs-number">-1</span>

            <span class="hljs-keyword">if</span> terminal <span class="hljs-keyword">or</span> epoch_ended:
                <span class="hljs-keyword">if</span> epoch_ended <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span>(terminal):
                    print(<span class="hljs-string">'Warning: trajectory cut off by epoch at %d steps.'</span>%ep_len, flush=<span class="hljs-literal">True</span>)
                <span class="hljs-comment"># if trajectory didn't reach terminal state, bootstrap value target</span>
                <span class="hljs-keyword">if</span> timeout <span class="hljs-keyword">or</span> epoch_ended:
                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))
                <span class="hljs-keyword">else</span>:
                    v = <span class="hljs-number">0</span>
                buf.finish_path(v)
                <span class="hljs-keyword">if</span> terminal:
                    print(<span class="hljs-string">'Episodic Return: {:.2f}, Episodic Length: {:.2f}'</span>.format(ep_ret, ep_len))
                o, ep_ret, ep_len = env.reset(), <span class="hljs-number">0</span>, <span class="hljs-number">0</span>

        <span class="hljs-comment"># Perform VPG update!</span>
        update()
        print(<span class="hljs-string">'Epoch: {}, TotalEnvInteracts: {}'</span>.format(epoch, (epoch+<span class="hljs-number">1</span>)*steps_per_epoch))
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> argparse
parser = argparse.ArgumentParser()
parser.add_argument(<span class="hljs-string">'--env'</span>, type=str, default=<span class="hljs-string">'MountainCarContinuous-v0'</span>)
parser.add_argument(<span class="hljs-string">'--hid'</span>, type=int, default=<span class="hljs-number">64</span>) <span class="hljs-comment"># number of neurons for hidden layer</span>
parser.add_argument(<span class="hljs-string">'--l'</span>, type=int, default=<span class="hljs-number">2</span>) <span class="hljs-comment"># number of hidden layers</span>
parser.add_argument(<span class="hljs-string">'--gamma'</span>, type=float, default=<span class="hljs-number">0.99</span>)
parser.add_argument(<span class="hljs-string">'--seed'</span>, <span class="hljs-string">'-s'</span>, type=int, default=<span class="hljs-number">0</span>)
parser.add_argument(<span class="hljs-string">'--steps'</span>, type=int, default=<span class="hljs-number">4000</span>)
parser.add_argument(<span class="hljs-string">'--epochs'</span>, type=int, default=<span class="hljs-number">500</span>)
args, unknown = parser.parse_known_args()

vpg(<span class="hljs-keyword">lambda</span> : gym.make(args.env), actor_critic=MLPActorCritic,
    ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, 
    seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,)
</div></code></pre>
<pre><code>Number of parameters: 	 pi: 4418, 	 v: 4417

Episodic Return: -29.95, Episodic Length: 999.00
Episodic Return: -29.77, Episodic Length: 999.00
Episodic Return: -30.82, Episodic Length: 999.00
Episodic Return: -31.10, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.
LossPi: -0.1057, LossV: 0.4318
Epoch: 0, TotalEnvInteracts: 4000
Episodic Return: -31.91, Episodic Length: 999.00
Episodic Return: -31.17, Episodic Length: 999.00
Episodic Return: -32.05, Episodic Length: 999.00
Episodic Return: -29.97, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.
LossPi: -0.0380, LossV: 0.4265
Epoch: 1, TotalEnvInteracts: 8000
Episodic Return: -31.23, Episodic Length: 999.00
Episodic Return: -31.77, Episodic Length: 999.00
Episodic Return: -30.81, Episodic Length: 999.00
Episodic Return: -30.77, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.
LossPi: -0.0442, LossV: 0.4196
Epoch: 2, TotalEnvInteracts: 12000
Episodic Return: -31.10, Episodic Length: 999.00
Episodic Return: -30.84, Episodic Length: 999.00
Episodic Return: -30.74, Episodic Length: 999.00
Episodic Return: -31.43, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.
LossPi: -0.0442, LossV: 0.4219
Epoch: 3, TotalEnvInteracts: 16000
Episodic Return: -31.07, Episodic Length: 999.00
Episodic Return: -31.80, Episodic Length: 999.00
Episodic Return: -32.52, Episodic Length: 999.00
Episodic Return: -31.45, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.
LossPi: -0.0418, LossV: 0.4225
Epoch: 4, TotalEnvInteracts: 20000
Episodic Return: -30.85, Episodic Length: 999.00
Episodic Return: -29.22, Episodic Length: 999.00
Episodic Return: -31.52, Episodic Length: 999.00
Episodic Return: -32.24, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.
LossPi: -0.0477, LossV: 0.4354
Epoch: 5, TotalEnvInteracts: 24000
Episodic Return: -30.87, Episodic Length: 999.00
Episodic Return: -32.10, Episodic Length: 999.00
Episodic Return: -29.42, Episodic Length: 999.00
Episodic Return: -31.16, Episodic Length: 999.00
Warning: trajectory cut off by epoch at 4 steps.



---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

Input In [18], in &lt;cell line: 12&gt;()
      9 parser.add_argument('--epochs', type=int, default=500)
     10 args, unknown = parser.parse_known_args()
---&gt; 12 vpg(lambda : gym.make(args.env), actor_critic=MLPActorCritic,
     13     ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, 
     14     seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,)


Input In [17], in vpg(env_fn, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, gamma, pi_lr, vf_lr, train_v_iters, lam, max_ep_len, logger_kwargs, save_freq)
    240         o, ep_ret, ep_len = env.reset(), 0, 0
    242 # Perform VPG update!
--&gt; 243 update()
    244 print('Epoch: {}, TotalEnvInteracts: {}'.format(epoch, (epoch+1)*steps_per_epoch))


Input In [17], in vpg.&lt;locals&gt;.update()
    199     vf_optimizer.zero_grad()
    200     loss_v = compute_loss_v(data)
--&gt; 201     loss_v.backward()
    202     vf_optimizer.step()
    204 print('LossPi: {:.4f}, LossV: {:.4f}'.format(loss_pi.item(), loss_v.item()))


File ~/miniforge3/envs/mujoco/lib/python3.8/site-packages/torch/_tensor.py:307, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)
    298 if has_torch_function_unary(self):
    299     return handle_torch_function(
    300         Tensor.backward,
    301         (self,),
   (...)
    305         create_graph=create_graph,
    306         inputs=inputs)
--&gt; 307 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)


File ~/miniforge3/envs/mujoco/lib/python3.8/site-packages/torch/autograd/__init__.py:154, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    151 if retain_graph is None:
    152     retain_graph = create_graph
--&gt; 154 Variable._execution_engine.run_backward(
    155     tensors, grad_tensors_, retain_graph, create_graph, inputs,
    156     allow_unreachable=True, accumulate_grad=True)


KeyboardInterrupt: 
</code></pre>
<pre class="hljs"><code><div>
</div></code></pre>

</body>
</html>
